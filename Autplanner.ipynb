{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinvrana/Documents/GitHub/coral/coral/analysis/_sequencing/needle.py:11: UserWarning: NW alignment extension could not be imported, falling backon native Python version (~100 times slower).\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from aqutils import scripts, cli\n",
    "from pydent.planner import Planner\n",
    "from pydent.browser import Browser\n",
    "from mysession import nursery, production, benchapi\n",
    "from tqdm import tqdm\n",
    "\n",
    "cli = cli.entrypoint()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvs = production.FieldValue.last(100)\n",
    "for fv in fvs:\n",
    "    fv.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache relationships and models\n",
    "OperationType <> FieldType <> AllowableFieldType relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Browser(production)\n",
    "\n",
    "ots = browser.where({\"deployed\": True}, \"OperationType\")\n",
    "browser.set_verbose(False)\n",
    "production.set_verbose(False)\n",
    "results = browser.recursive_retrieve(ots, {\n",
    "    \"field_types\": {\n",
    "        \"allowable_field_types\": {\n",
    "            \"object_type\": [],\n",
    "            \"sample_type\": [],\n",
    "            \"field_type\": []\n",
    "        }\n",
    "    }\n",
    "}\n",
    ")\n",
    "\n",
    "fts = results['field_types']\n",
    "inputs = [ft for ft in fts if ft.role == 'input']\n",
    "outputs = [ft for ft in fts if ft.role == 'output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [00:00<00:00, 81472.23it/s]\n",
      "100%|██████████| 563/563 [00:00<00:00, 128273.84it/s]\n"
     ]
    }
   ],
   "source": [
    "def external_aft_hash(aft):\n",
    "    if not aft.field_type:\n",
    "        return str(uuid4())\n",
    "    if aft.field_type.part:\n",
    "        part = True\n",
    "    else:\n",
    "        part = False\n",
    "    return \"{object_type}-{sample_type}-{part}\".format(\n",
    "        object_type=aft.object_type_id,\n",
    "        sample_type=aft.sample_type_id,\n",
    "        part=part,\n",
    "    )\n",
    "\n",
    "def internal_aft_hash(aft):\n",
    "    return \"{operation_type}\".format(\n",
    "        operation_type=aft.field_type.parent_id,\n",
    "        routing=aft.field_type.routing,\n",
    "        sample_type=aft.sample_type_id\n",
    "    )\n",
    "\n",
    "fts = [ft for ft in results['field_types'] if ft.ftype == 'sample']\n",
    "inputs = [ft for ft in fts if ft.role == 'input']\n",
    "outputs = [ft for ft in fts if ft.role == 'output']\n",
    "\n",
    "input_afts = []\n",
    "for i in inputs:\n",
    "    for aft in i.allowable_field_types:\n",
    "        if aft not in input_afts:\n",
    "            input_afts.append(aft)\n",
    "\n",
    "output_afts = []\n",
    "for o in outputs:\n",
    "    for aft in o.allowable_field_types:\n",
    "        if aft not in output_afts:\n",
    "            output_afts.append(aft)\n",
    "all_afts = input_afts + output_afts\n",
    "\n",
    "# input_by_object_type_id = {}\n",
    "# output_by_object_type_id = {aft.object_type_id: aft for aft in output_aft}\n",
    "import itertools\n",
    "production.set_verbose(True)\n",
    "\n",
    "external_groups = {}\n",
    "for aft in input_afts:\n",
    "    external_groups.setdefault(external_aft_hash(aft), []).append(aft)\n",
    "    \n",
    "    \n",
    "internal_groups = {}\n",
    "for aft in output_afts:\n",
    "    internal_groups.setdefault(internal_aft_hash(aft), []).append(aft)\n",
    "    \n",
    "    \n",
    "# from tqdm import tqdm\n",
    "matching = []\n",
    "for oaft in tqdm(output_afts):\n",
    "    hsh = external_aft_hash(oaft)\n",
    "    externals = external_groups.get(hsh, [])\n",
    "    for aft in externals:\n",
    "        matching.append((oaft, aft))\n",
    "\n",
    "for iaft in tqdm(input_afts):\n",
    "    hsh = internal_aft_hash(iaft)\n",
    "    internals = internal_groups.get(hsh, [])\n",
    "    for aft in internals:\n",
    "        matching.append((iaft, aft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge weights\n",
    "\n",
    "Here we use historical data for how plans are 'normally' connected and use that information to automatically plan experiments in the future.\n",
    "\n",
    "Certainly, something like a *preference file* or a *specification file* how how thing can/should be connected would be exteremely useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 14:45:00,866 - AqHTTP@http://52.27.43.242/ - INFO - BODY: {'arguments': {},\n",
      " 'method': 'where',\n",
      " 'model': 'Plan',\n",
      " 'options': {'limit': '300', 'offset': '-1', 'reverse': 'True'}}\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "plans = browser.last(300, \"Plan\")\n",
    "browser.set_verbose(False)\n",
    "browser.session.set_verbose(False)\n",
    "browser.recursive_retrieve(plans, {\"operations\": {\"field_values\": [\"allowable_field_type\", \"wires_as_source\", \"wires_as_dest\"]}}, strict=False)\n",
    "for p in plans:\n",
    "    p.wires\n",
    "\n",
    "all_wires = reduce((lambda x, y: x + y), [p.wires for p in plans])\n",
    "all_operations = reduce((lambda x, y: x + y), [p.operations for p in plans])\n",
    "browser.recursive_retrieve(all_wires[:], {\n",
    "    \"source\": {\n",
    "        \"field_type\": [],\n",
    "        \"operation\": \"operation_type\",\n",
    "        \"allowable_field_type\": []\n",
    "    },\n",
    "    \"destination\": {\n",
    "        \"field_type\": [],\n",
    "        \"operation\": \"operation_type\",\n",
    "        \"allowable_field_type\": []\n",
    "    }\n",
    "}, strict=False)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10834/10834 [00:00<00:00, 64862.38it/s]\n"
     ]
    }
   ],
   "source": [
    "production.set_verbose(True)\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "def hash_afts(aft1, aft2):\n",
    "    source_hash = external_aft_hash(aft1)\n",
    "    dest_hash = external_aft_hash(aft2)\n",
    "    return \"{}->{}\".format(source_hash, dest_hash)\n",
    "    return str(uuid4())\n",
    "\n",
    "def hash_wire(wire):\n",
    "    func = external_aft_hash\n",
    "    ifv = wire.source\n",
    "    ofv = wire.destination\n",
    "    if ifv and ofv:\n",
    "        return hash_afts(ofv.allowable_field_type, ifv.allowable_field_type)\n",
    "    return str(uuid4())\n",
    "\n",
    "wire_hash_count = {}\n",
    "wire_source_hash_count = {}\n",
    "\n",
    "aft_pairs = []\n",
    "for wire in all_wires:\n",
    "    if wire.source and wire.destination:\n",
    "        aft_pairs.append((wire.source.allowable_field_type, wire.destination.allowable_field_type))\n",
    "for op in all_operations:\n",
    "    for i in op.inputs:\n",
    "        for o in op.outputs:\n",
    "            aft_pairs.append((i.allowable_field_type, o.allowable_field_type))\n",
    "\n",
    "for aft1, aft2 in tqdm(aft_pairs):\n",
    "    if aft1 and aft2:\n",
    "        aft_hash = hash_afts(aft1, aft2)\n",
    "        wire_hash_count.setdefault(aft_hash, 0)\n",
    "        wire_hash_count[aft_hash] += 1\n",
    "\n",
    "        source_hash = external_aft_hash(aft1)\n",
    "        wire_source_hash_count.setdefault(source_hash, 0)\n",
    "        wire_source_hash_count[source_hash] += 1\n",
    "\n",
    "def weight_edge(n1, n2):\n",
    "    p = 1e-4\n",
    "    t = 0\n",
    "    if n1 and n1:\n",
    "        n = wire_hash_count.get(hash_afts(n1, n2))*1.0\n",
    "        t = wire_source_hash_count.get(external_aft_hash(n1))*1.0\n",
    "\n",
    "    if t > 0:\n",
    "        p = n/t\n",
    "    w = (1-p)/(1+p)\n",
    "    return 10/(1.0001 - w)\n",
    "\n",
    "\n",
    "examples = []\n",
    "for w in all_wires[:]:\n",
    "    if w.source and w.destination:\n",
    "        weight = weight_edge(w.source.allowable_field_type, w.destination.allowable_field_type)\n",
    "        if weight > 0 and weight < 0.9:\n",
    "            examples.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.018841051723879\n",
      "15.018841051723879\n",
      "15.018841051723879\n",
      "14.99775033744938\n",
      "14.99775033744938\n",
      "14.99775033744938\n",
      "14.99775033744938\n",
      "14.99775033744938\n",
      "14.99775033744938\n",
      "14.99775033744938\n"
     ]
    }
   ],
   "source": [
    "for aft1, aft2 in matching[:10]:\n",
    "    print(weight_edge(aft1, aft2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search graph\n",
    "\n",
    "While there are many ways to connect Operations together that takes in a Fragment Stock and eventually produces a Yeast Glycerol Stock, many of these workflows do not make sense from a experimental standpoint. These *non-sensical* workflows are simply not what people do in the lab to produce a yeast glycerol stock.\n",
    "\n",
    "As an example, it may be unconventional to connect a `Yeast Transformation` operation (which produces a yeast plate) directly to a `Yeast Overnight` operation (which produces a yeast overnight culture) since people typically check the plates first using `Chect Yeast Plate` before making a overnight culture. Even though the Transformation > Overnight path has fewer steps, this is far less common than the Transformation > Check Plate > Overnight path. During this search, we want to capture this concept.\n",
    "\n",
    "To do this, we use historical data from past completed plans. In this example, say there 1000 `Yeast Transformations` in our historical data set with only 10 of these being connected directly to `Overnight` and the other 990 connected to `Check Plates`. We use these ratios (p=0.01 and p=0.99) to compute weights for the network of all possibly connected operations with less likely connections having a greater weight than other connections.\n",
    "\n",
    "Example: Find the shortest path between a start and end nodes. Here, we show an example of the shortest path to take between a *Fragment Stock* and a *Yeast Glycerol Stock*.\n",
    "\n",
    "Results: Given the OperationTypes in the Aquarium database, it would be most typical to product a plasmid from fragments in a gibson assembly. This plasmid would be miniprepped and transformed into yeast. Here we find this is the case. Without historical data, the search returns a much shorter and non-sensical workflow (Yeast Transformation to Define Culture Conditions, which, experimentally, is likely not our intention).\n",
    "\n",
    "Note: The AutoPlanner computes probability of connecting different AllowableFieldTypes together, which accounts for external Operation to Operation connections as well as internal input to output connections within the same operation (it may be unconventional to use Fragment Stocks to produce another Fragment Stock in a Golden Gate Assembly, for example).\n",
    "\n",
    "### Populating the graph\n",
    "\n",
    "Aquarium plans are not experimentally valid unless all inputs in every operation has either an incoming wire or item, and all field values are populated with either a parameter or sample. We need to populate the graph with specific samples and items in order to autoplan an experiment.\n",
    "\n",
    "To make a plan network, we do the following:\n",
    "\n",
    "1. We add a START and GOAL node to the network.\n",
    "2. We add edges from every node (AllowableFieldType) to its available items (location != deleted) that match its sample_type_id and object_type_id. **edge_weight=0**\n",
    "3. Add edges from every item to the START node. **edge_weight=0**\n",
    "4. We add undirected *sister edges* between relavant nodes. Sister edges occur between allowable_field_types that are attached to input FieldTypes within the same OperationType whose routing information is valid. Routing information between two afts are valid if either:\n",
    "    * (a) the aft.field_type.routing is different between the two afts or \n",
    "    * (b) aft.sample_type_id between the two afts are the same.\n",
    "\n",
    "**Goal**\n",
    "Find the least costly valid plan.\n",
    "\n",
    "**Definition of valid plan**\n",
    "A plan is valid if (a) its network has no cycles, (b) the network has a single leaf node (including the *sister edges*)\n",
    "\n",
    "#### Notes on Populating with items\n",
    "\n",
    "It is far to intensive to query all items of a allowable_field_types. This should occur only after additional sample filtering has occured. This will mean establishing sample construction in cases where sample routing is switched along an edge. This can mean specifying how sample_types and operation_types interact?\n",
    "\n",
    "# DONT FORGET TO INCLUDE PARTS/COLLECTIONS\n",
    "\n",
    "### Dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct template graph\n",
    "\n",
    "A template graph represents **all** possible workflows. Each node is an AllowableFieldType. Connections are determined by:\n",
    "\n",
    "1. internal connections where \"input-{operation_type_id}\" -> \"output-{operation_type_id}\"\n",
    "2. external connections where \"output-{object_type_id}-{sample_type_id}\" -> \"input-{object_type_id}-{sample_type_id}\"\n",
    "\n",
    "Weights are determined as described previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a75d15acd7ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maft1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maft2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatching\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maft1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maft2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maft1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maft2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maft1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maft2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building Graph:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-51e863c16f3a>\u001b[0m in \u001b[0;36mweight_edge\u001b[0;34m(n1, n2)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mn1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwire_hash_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash_afts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwire_source_hash_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexternal_aft_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# new direction graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# add weighted edges\n",
    "for aft1, aft2 in matching:\n",
    "    if aft1 and aft2:\n",
    "        G.add_edge(aft1.id, aft2.id, weight=weight_edge(aft1, aft2))\n",
    "\n",
    "print(\"Building Graph:\")\n",
    "print(\"  {} edges\".format(len(matching)))\n",
    "print(\"  {} nodes\".format(len(G)))\n",
    "\n",
    "all_afts = input_afts + output_afts\n",
    "\n",
    "ignore_ots = browser.where({\"category\": \"Control Blocks\"}, \"OperationType\")\n",
    "nodes = [aft.id for aft in all_afts if aft.field_type.parent_id and aft.field_type.parent_id not in [ot.id for ot in ignore_ots]]\n",
    "subgraph = G.subgraph(nodes)\n",
    "print(\"Graph size reduced from {} to {} nodes\".format(len(G), len(subgraph)))\n",
    "print(\"Example edges\")\n",
    "for e in list(subgraph.edges)[:3]:\n",
    "    aft1 = browser.find(e[0], \"AllowableFieldType\")\n",
    "    aft2 = browser.find(e[1], \"AllowableFieldType\")\n",
    "    print()\n",
    "    print(\"{} {}\".format(aft1.field_type.role, aft1))\n",
    "    print(\"{} {}\".format(aft2.field_type.role, aft2))\n",
    "    \n",
    "for edge in list(subgraph.edges)[:5]:\n",
    "    print(subgraph[edge[0]][edge[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guess sample compisition for sample properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frags = browser.search(\"level3\")\n",
    "example_sample = frags[0]\n",
    "\n",
    "browser.set_verbose(False)\n",
    "\n",
    "def collect_sample_dependencies(samples, edges=None):\n",
    "    if edges is None:\n",
    "        edges = []\n",
    "    results = browser.recursive_retrieve(samples, {'field_values': 'sample'})\n",
    "    retrieved_samples = results['sample']\n",
    "    if not retrieved_samples:\n",
    "        return edges\n",
    "    else:\n",
    "        edges += collect_sample_dependencies(retrieved_samples, edges=edges[:])\n",
    "    for s in samples:\n",
    "        for fv in s.field_values:\n",
    "            if fv.sample:\n",
    "                edge = (s.id, fv.sample.id)\n",
    "                if edge not in edges:\n",
    "                    edges.append(edge)\n",
    "    return edges\n",
    "\n",
    "def possible_items(samples, allowable_field_types):\n",
    "    edges = collect_sample_dependencies(samples)\n",
    "    sample_ids = reduce(lambda x, y: x + y, [list(x) for x in edges])\n",
    "    samples = browser.find(sample_ids)\n",
    "    for s in samples:\n",
    "        print(s)\n",
    "    object_type_ids = list(reduce((lambda x, y: set(x).union(set(y))), [[aft.object_type_id] for aft in all_afts]))\n",
    "    potential_items = production.Item.where({\"object_type_id\": object_type_ids, \"sample_id\": sample_ids})\n",
    "    return [item for item in potential_items if item.location != 'deleted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sister edges\n",
    "\n",
    "sister_edges = []\n",
    "\n",
    "by_ot = {}\n",
    "for aft in input_afts:\n",
    "    by_ot.setdefault(aft.field_type.parent_id, []).append(aft)\n",
    "    \n",
    "for afts in by_ot.values():\n",
    "    for aft1 in afts:\n",
    "        for aft2 in afts:\n",
    "            if not aft1.id != aft2.id:\n",
    "                if  aft1.sample_type_id == aft1.sample_type_id or aft1.field_type.routing != aft2.field_type.routing:\n",
    "                    sister_edges.append((aft1, aft2))\n",
    "len(sister_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_type_ids = list(reduce((lambda x, y: set(x).union(set(y))), [[aft.object_type_id] for aft in all_afts]))\n",
    "sample_id = 24420\n",
    "production.Item.where({\"object_type_id\": object_type_ids, \"sample_id\": sample_id})\n",
    "# browser.where({\"object_type_id\": object_type_ids[-2:]}, \"Item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pydent.utils import filter_list\n",
    "production.set_verbose(False)\n",
    "\n",
    "graph = nx.DiGraph()\n",
    "graph.add_nodes_from(subgraph.nodes(data=True))\n",
    "graph.add_edges_from(subgraph.edges(data=True))\n",
    "\n",
    "goal_object_type = \"Yeast Glycerol Stock\"\n",
    "goal_sample = 'CEN.PK2 - MAT A ::: pMOD4G-dcas9-mxi1 | PMOD6-PGRR-F10-yeGFP _X_ CEN.PK2 - MAT alpha ::: pMODKan-HO-pACT1-ZEV4 | pMOD8-pGALZ4-URGR-F10 (JV)'\n",
    "\n",
    "obj1 = production.ObjectType.find_by_name(\"Fragment Stock\")\n",
    "obj2 = production.ObjectType.find_by_name(\"Yeast Glycerol Stock\")\n",
    "\n",
    "afts1 = filter_list(input_afts, object_type_id=obj1.id, sample_type_id=obj1.sample_type_id)\n",
    "afts2 = filter_list(output_afts, object_type_id=obj2.id, sample_type_id=obj2.sample_type_id)\n",
    "\n",
    "# add item nodes\n",
    "# items = possible_items([browser.find_by_name(goal_sample)], input_afts + output_afts)\n",
    "# print(\"{} available items found\".format(len(items)))\n",
    "\n",
    "graph.add_node(\"START\")\n",
    "\n",
    "# item_node = lambda item: \"Item{}\".format(item.id)\n",
    "# item_by_obj_id = {}\n",
    "# for item in items:\n",
    "#     item_by_obj_id.setdefault(item.object_type_id, []).append(item)\n",
    "\n",
    "# for aft in all_afts:\n",
    "#     for item in item_by_obj_id.get(aft.object_type_id, []):\n",
    "#         graph.add_edge(item_node(item), aft.id, weight=0)\n",
    "\n",
    "# for item in items:\n",
    "#     graph.add_node(item_node(item), item=item)\n",
    "#     graph.add_edge(\"START\", item_node(item), weight=0)\n",
    "for aft in input_afts:\n",
    "    graph.add_edge(\"START\", aft.id)\n",
    "\n",
    "\n",
    "graph.add_node(\"END\")\n",
    "for aft in afts2:\n",
    "    graph.add_edge(aft.id, \"END\", weight=0)\n",
    "\n",
    "start_nodes = [aft.id for aft in afts1 if aft.id in nodes]\n",
    "end_nodes = [aft.id for aft in afts2 if aft.id in nodes]\n",
    "\n",
    "print(\"Input afts:\")\n",
    "for aftid in start_nodes:\n",
    "    aft = browser.find(aftid, 'AllowableFieldType')\n",
    "    try:\n",
    "        print(aft.field_type.operation_type)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "allpaths = []\n",
    "print(list(graph.successors(\"START\")))\n",
    "for n1 in tqdm(list(graph.successors(\"START\"))):\n",
    "    for n2 in [\"END\"]:\n",
    "        try:\n",
    "            path = nx.dijkstra_path(graph, n1, n2, weight='weight')\n",
    "            path_length = nx.dijkstra_path_length(graph, n1, n2, weight='weight')\n",
    "            allpaths.append((path, path_length))\n",
    "        except nx.exception.NetworkXNoPath:\n",
    "            pass\n",
    "            \n",
    "allpaths = sorted(allpaths, key=lambda x: x[1])\n",
    "\n",
    "print()\n",
    "print(\"*\"*50)\n",
    "print(\"{} >> {}\".format(obj1.name, obj2.name))\n",
    "print(\"*\"*50)\n",
    "print()\n",
    "for path, pathlen in allpaths[:10]:\n",
    "    print(path)\n",
    "    ots = []\n",
    "    for aftid in path:\n",
    "        aft = browser.find(aftid, 'AllowableFieldType')\n",
    "        if aft:\n",
    "            ot = browser.find(aft.field_type.parent_id, 'OperationType')\n",
    "            ots.append(\"{ot}\".format(role=aft.field_type.role, name=aft.field_type.name, ot=ot.name))\n",
    "        edge_weights = [graph[x][y]['weight'] for x, y in zip(path[:-1], path[1:])]\n",
    "    print(pathlen)\n",
    "    print(edge_weights)\n",
    "    print(len(ots))\n",
    "    print(ots)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[   0.        ,   15.19976896,   30.21387018, ...,           inf,\n",
       "                   inf,           inf],\n",
       "        [ 122.83990499,    0.        ,   15.01410122, ...,           inf,\n",
       "                   inf,           inf],\n",
       "        [ 107.82580377,  123.02557273,    0.        , ...,           inf,\n",
       "                   inf,           inf],\n",
       "        ..., \n",
       "        [          inf,           inf,           inf, ...,    0.        ,\n",
       "                   inf,           inf],\n",
       "        [          inf,           inf,           inf, ...,           inf,\n",
       "            0.        ,           inf],\n",
       "        [          inf,           inf,           inf, ...,           inf,\n",
       "                   inf,    0.        ]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.floyd_warshall_numpy(graph, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
